{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4ILw10UUJadDjT5+zMPmp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choudaryhussainali/MCQ_Grading_Bot/blob/main/MCQ_Grading_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5IWA0mnm31id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fd7b2e-18d0-40e4-ebc2-4380e20a67ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gradio Pillow groq google-generativeai python-dotenv --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# For Groq\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')  # Set this in Colab secrets\n",
        "\n",
        "# For Gemini\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')  # Set this in Colab secrets\n"
      ],
      "metadata": {
        "id": "iZXHkU1v-JSG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import io\n",
        "import time\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "def grade_mcqs_from_image(image):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Prepare the image\n",
        "    img_byte_arr = io.BytesIO()\n",
        "    image.save(img_byte_arr, format='PNG')\n",
        "    img_bytes = img_byte_arr.getvalue()\n",
        "\n",
        "    # Optimized prompt for Gemini Vision\n",
        "    prompt = \"\"\"Carefully analyze this image of a solved MCQ exam paper. Extract and return the following information in JSON format:\n",
        "\n",
        "    {\n",
        "        \"student_name\": \"[Name from paper or 'Unknown']\",\n",
        "        \"total_questions\": [Total number of questions],\n",
        "        \"correct_answers\": [Number of correct answers],\n",
        "        \"wrong_answers\": [Number of wrong answers],\n",
        "        \"score_percentage\": [Percentage score],\n",
        "        \"mcqs\": [\n",
        "            {\n",
        "                \"question_number\": \"[Q1, Q2, etc.]\",\n",
        "                \"question\": \"[Full question text]\",\n",
        "                \"correct_answer\": \"[Correct option]\",\n",
        "                \"student_answer\": \"[Student's chosen option]\",\n",
        "                \"is_correct\": [true/false]\n",
        "            },\n",
        "            ... (repeat for all questions)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    Important guidelines:\n",
        "    1. Focus on accuracy - double-check each answer\n",
        "    2. For student_name, look for any name written on the paper\n",
        "    3. Clearly distinguish between correct answers and student answers\n",
        "    4. Return ONLY valid JSON, no additional text or explanations\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Send to Gemini for processing\n",
        "        response = model.generate_content(\n",
        "            [prompt, Image.open(io.BytesIO(img_bytes))],\n",
        "            generation_config={\n",
        "                \"temperature\": 0.1,\n",
        "                \"max_output_tokens\": 4000\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Clean the response to extract pure JSON\n",
        "        response_text = response.text.replace('```json', '').replace('```', '').strip()\n",
        "        result = json.loads(response_text)\n",
        "\n",
        "        # Calculate processing time\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        # Format the results for display\n",
        "        formatted_output = f\"⏱ Processed in {processing_time:.2f} seconds\\n\\n\"\n",
        "        formatted_output += f\"👤 Student: {result.get('student_name', 'Unknown')}\\n\"\n",
        "        formatted_output += f\"📝 Total Questions: {result['total_questions']}\\n\"\n",
        "        formatted_output += f\"✅ Correct: {result['correct_answers']} | ❌ Wrong: {result['wrong_answers']}\\n\"\n",
        "        formatted_output += f\"📊 Score: {result['score_percentage']}%\\n\\n\"\n",
        "        formatted_output += \"Detailed Results:\\n\"\n",
        "\n",
        "        for mcq in result['mcqs']:\n",
        "            status = \"✓\" if mcq['is_correct'] else \"✗\"\n",
        "            formatted_output += f\"\\n{mcq['question_number']}. {mcq['question']}\\n\"\n",
        "            formatted_output += f\"   Correct: {mcq['correct_answer']} | Student: {mcq['student_answer']} {status}\\n\"\n",
        "\n",
        "        return formatted_output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error processing image: {str(e)}\\n\\nPlease ensure:\\n1. Image is clear and well-lit\\n2. Answers are clearly marked\\n3. Text is readable\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(title=\"MCQ Grading Bot\") as demo:\n",
        "    gr.Markdown(\"## 📝 Automated MCQ Grading with Gemini Vision\")\n",
        "    gr.Markdown(\"Upload an image of a solved MCQ answer sheet for automatic grading\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(label=\"Upload Answer Sheet\", type=\"pil\")\n",
        "            submit_btn = gr.Button(\"Grade Answers\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(label=\"Grading Results\", lines=20, interactive=False)\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=grade_mcqs_from_image,\n",
        "        inputs=[image_input],\n",
        "        outputs=[output]\n",
        "    )\n",
        "\n",
        "# Launch the app with proper Colab handling\n",
        "try:\n",
        "    from google.colab import output\n",
        "    # Use standard Gradio Colab launch\n",
        "    demo.launch(debug=True, share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Running outside Colab: {str(e)}\")\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "LJa0Zhh3Hj7r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "403e67a6-3c1c-4649-892d-e2b71d12687b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8e06a082800aa5c9b8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8e06a082800aa5c9b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8e06a082800aa5c9b8.gradio.live\n"
          ]
        }
      ]
    }
  ]
}